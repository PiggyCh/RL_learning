## RL MC with exploring starts for gym Blackjack (gym 0.22.0)
import gym
import gym
import time
import random
import numpy as np
import copy
from gym.envs.toy_text.blackjack import BlackjackEnv
env = BlackjackEnv()
env_new=env.reset()
print(env_new)
def greedy_policy(Q_table,obs):#obs是一个元组，为3维， Q_table 为4维
    index=(obs[0],obs[1],int(obs[2]))
    return np.argmax(Q_table[index])
def get_prob(epsilon,Q_table,obs,action):
    if greedy_policy(Q_table,obs)==action:
        return (1/(1-epsilon+epsilon/2)) #这里的2就是动作空间
    else:
        return 0 #如果不是贪婪动作就不需要算权重了。。。
def MC_evaluation_update(data,Q_table,iteration_weight_Q,epsilon):#这里为了避免记录每一幕数据来求平均G，采取了增量式，用iteration_Q记录迭代次数。
    G=0
    gamma=1
    obs,rewards,actions=data['obs'],data['rewards'],data['actions']
    W=1
    old_Q_table=copy.copy(Q_table)
    for i in range(len(obs)-1,-1,-1): #采取是倒序计算，更方便
        G= G*gamma + rewards[i]
        if obs[i] in obs[:i] and actions[i] in actions[:i]: #首次访问 ，只记录第一次
            continue
        index= (obs[i][0],obs[i][1],int(obs[i][2]),actions[i]) #操作的索引值
        iteration_weight_Q[index] += W
        Q_table[index]+=(W/iteration_weight_Q[index])*(G-Q_table[index]) #迭代式 Q<-Q+1/k(Q-G)
        fac = get_prob(epsilon,old_Q_table,obs[i],actions[i])
        if fac:
            W = W*fac
    return Q_table,iteration_weight_Q
def sample_data(env,Q_table,epsilon=0.0,random_policy=False):
    obs,reward,actions=[],[],[]
    observation = env.reset()
    obs.append(observation)
    done = False
    while not done:
        if random_policy:
            action=random.randint(0,1)
        else:
            if random.random() < epsilon:  # random.random()是0-1的均匀分布，当出现小于0.05时采取随即动作
                action = random.randint(0, 1)
            else:
                action = greedy_policy(Q_table,observation)
        observation,r,done,_ = env.step(action)
        if not done:
            obs.append(observation)
        actions.append(action)
        reward.append(r)
    return {'obs':obs,
            'actions':actions,
            'rewards':reward}
def training(env,epsilon=0.0,iteration_time=50000):
    obs_space=env.observation_space
    action_space=env.action_space
    Q_shape = [item.n for item in obs_space]+[action_space.n]
    Q_table = np.zeros(Q_shape)
    iteration_weight_Q = np.zeros(Q_shape) #需要维护累计权重C
    for _ in range(iteration_time):
        data = sample_data(env,Q_table,epsilon)
        Q_table,iteration_Q = MC_evaluation_update(data,Q_table,iteration_weight_Q,epsilon)
    return Q_table
def testing(Q_table,epsilon=0,random_policy=False,testing_time=100000):
    win,draw,lose=0,0,0
    for i in range(testing_time):
        data = sample_data(env,Q_table,0,random_policy)
        if data['rewards'][-1]>0:
            win+=1
        elif data['rewards'][-1]<0:
            lose+=1
        else:
            draw+=1
    print('-------------------------------epsilon: '+str(epsilon)+'------------------------------------')
    print('win: '+str(win)+' lose: '+str(lose)+' draw game: ' +str(draw))
    print('win_rate: '+str(win/testing_time)[:4]+' lose_rate: '+str(lose/testing_time)[:4]+' draw_rate: '+str(draw/testing_time)[:4])

epsilon=[0,0.05,0.10,0.15,0.20,0.5,0.8,1.0]
for val in epsilon:
    Q_table=training(env,epsilon=val,iteration_time=100000)
    testing(Q_table,epsilon=val)
# Q_table=training(env,iteration_time=100000)
# data_1,data_2=Q_table[:,:,0,0],Q_table[:,:,0,1]
# data_3, data_4 = Q_table[:, :, 1, 0], Q_table[:, :, 1, 1]
# testing(Q_table)
# testing(Q_table,random_policy=True)
